import numpy as np
import pandas as pd
from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity
from factor_analyzer import FactorAnalyzer
from scipy.stats import zscore

def drop_low_mean(df: pd.DataFrame, threshold: float):
    # åªä¿ç•™æ•°å€¼åˆ—
    df_num = df.select_dtypes(include=["number"])
    mean_values = df_num.mean()
    to_drop = mean_values[mean_values < threshold].index
    return df_num.drop(columns=to_drop), list(to_drop)

def drop_high_corr(df: pd.DataFrame, threshold: float):
    # åªä¿ç•™æ•°å€¼åˆ—
    df_num = df.select_dtypes(include=["number"])
    c = df_num.corr(method="pearson").abs()
    upper = c.where(np.triu(np.ones(c.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    return df_num.drop(columns=to_drop), to_drop, c

def eigenvalues_for(df: pd.DataFrame):
    fa0 = FactorAnalyzer(rotation=None)
    fa0.fit(df)
    EigenValue, value = fa0.get_eigenvalues()
    return EigenValue

def select_factor_number(eigs, adjust=0):
    return max(1, int((eigs > 1).sum()) - int(adjust))

def retain_max_abs_value_row(row):
    row = pd.to_numeric(row, errors="coerce")
    abs_row = row.abs()
    max_index = abs_row.idxmax(skipna=True)
    return row.where(row.index == max_index)

def run_factor_pipeline(features_df: pd.DataFrame,
                        mean_threshold: float = 0.0,
                        corr_threshold: float = 0.95,
                        communality: float = 0.4,
                        method: str = "uls",
                        rotation: str = "promax",
                        cum_explain: float = 0.60,
                        loop_input: str = "/"):
    # 1. å¤åˆ¶æ•°æ®å¹¶æ¸…æ´—ï¼ˆç§»é™¤å…¨ç©ºåˆ—ï¼‰
    Data = features_df.copy()
    Data = Data.loc[:, Data.notna().any()]

    # ğŸš¨ å…³é”®ä¿®æ”¹ï¼šåªä¿ç•™æ•°å€¼å‹åˆ—
    Data = Data.select_dtypes(include=["number"])

    # ç§»é™¤æ–¹å·®ä¸º 0 çš„åˆ—
    Data = Data.loc[:, Data.var() > 0]

    # åˆ é™¤å…¨ç©ºåˆ—
    Data = Data.dropna(axis=1, how="all")

    # å¡«å……æ®‹ä½™ NaN
    Data = Data.fillna(0)

    # æ›¿æ¢ inf/-inf
    Data = Data.replace([np.inf, -np.inf], 0)

    if Data.shape[1] == 0:
        raise ValueError("ä¸Šä¼ çš„æ•°æ®ä¸­æ²¡æœ‰å¯ç”¨çš„æ•°å€¼å‹ç‰¹å¾ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")

    # 2. å‡å€¼ç­›é€‰
    Data, drop_mean = drop_low_mean(Data, mean_threshold)

    # 3. é«˜ç›¸å…³ç‰¹å¾ç­›é€‰
    Data, drop_corr, corr_mat = drop_high_corr(Data, corr_threshold)

    # 4. å‡†å¤‡å¾ªç¯å‰”é™¤ communality è¿‡ä½é¡¹
    if loop_input == "/":
        loop_times = -1
    elif loop_input == "0":
        loop_times = 0
    else:
        loop_times = int(loop_input)
    current_loop = 0

    fa = FactorAnalyzer(method=method, rotation=rotation)
    fa.fit(Data)
    new_Data = Data.copy()
    dropped_features_list = []

    while True and loop_input != "0" and (loop_times == -1 or current_loop < loop_times):
        current_loop += 1
        communalities = pd.DataFrame(fa.get_communalities(), index=new_Data.columns)
        features_comm = list(communalities[communalities[0] > communality].index)
        dropped = list(set(new_Data.columns) - set(features_comm))
        dropped_features_list.extend(dropped)
        if len(features_comm) == len(new_Data.columns):
            break
        new_Data = new_Data[features_comm]
        fa.fit(new_Data)

    # KMO / Bartlett
    kmo_all, kmo_model = calculate_kmo(new_Data)
    chi_square_value, p_value = calculate_bartlett_sphericity(new_Data)
    num_vars = new_Data.shape[1]
    df_bartlett = num_vars * (num_vars - 1) // 2

    # Eigenvalues and number of factors
    eigs = eigenvalues_for(new_Data)
    factor_number = select_factor_number(np.array(eigs), adjust=0)

    # æå–å› å­
    fa = FactorAnalyzer(n_factors=factor_number, method=method, rotation=rotation)
    fa.fit(new_Data)

    fac_loadings = pd.DataFrame(
        fa.loadings_,
        columns=[f"FAC{i}" for i in range(1, factor_number+1)],
        index=new_Data.columns
    )

    # å› å­è½½è·ç›¸å…³
    loadings_corr = fac_loadings.corr(method="pearson")

    # è§£é‡Šæ–¹å·®è¡¨
    idx = ["SS Loadings", "Proportion Variance", "Cumulative Variance"]
    df_variance = pd.DataFrame(
        data=fa.get_factor_variance(),
        index=idx,
        columns=[f"FAC{i}" for i in range(1, factor_number+1)]
    )

    # æ„å»ºæ¨¡å¼çŸ©é˜µï¼ˆä¿ç•™æ¯è¡Œç»å¯¹å€¼æœ€å¤§é¡¹ï¼‰
    pattern_matrix = fac_loadings.copy().fillna(0)
    pattern_matrix = pattern_matrix.apply(retain_max_abs_value_row, axis=1).dropna(how="all")
    sort_columns = pattern_matrix.columns.tolist()
    pattern_matrix = pattern_matrix.sort_values(by=sort_columns, ascending=[False]*len(sort_columns))

    # è®¡ç®— Z åˆ†æ•°
    normalized_data = new_Data.apply(zscore, axis=0)
    ordered_columns = pattern_matrix.index
    normalized_data = normalized_data[ordered_columns]

    # æ­£è´Ÿå· dict
    data_dict = {col: {idx: ("positive" if val > 0 else "negative") for idx, val in pattern_matrix[col].dropna().items()}
                 for col in pattern_matrix.columns}

    factor_score = pd.DataFrame(index=normalized_data.index, columns=pattern_matrix.columns)
    for i in factor_score.index:
        for key, mapping in data_dict.items():
            adjusted_values = []
            for col, sign in mapping.items():
                if col in normalized_data.columns:
                    v = normalized_data.loc[i, col]
                    adjusted_values.append(v if sign == "positive" else -v)
            if adjusted_values:
                factor_score.at[i, key] = sum(adjusted_values) / len(adjusted_values)
    factor_score = factor_score.astype(float)

    results = dict(
        data_after_filters=new_Data,
        dropped_low_mean=drop_mean,
        dropped_high_corr=drop_corr,
        corr_matrix=corr_mat,
        eigenvalues=eigs,
        kmo=kmo_model,
        bartlett=(chi_square_value, p_value),
        df_bartlett=df_bartlett,
        variance_table=df_variance,
        pattern_matrix=pattern_matrix,
        factor_scores=factor_score,
        loadings_corr=loadings_corr,
    )
    return results